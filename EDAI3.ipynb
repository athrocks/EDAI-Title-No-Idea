{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0_K1B8v9SjW",
    "outputId": "c45b192e-1e33-4878-b76f-d321581023db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oil_mine_presence\n",
      "0    210772\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"preprocessed_mining_data.csv\")\n",
    "\n",
    "# Define conditions for oil presence\n",
    "conditions = (\n",
    "    (df['P Wave Velocity (km/s)'] < 4.5) &   # Lower P-wave velocity\n",
    "    (df['S Wave Velocity (km/s)'] < 2.5) &   # Lower S-wave velocity\n",
    "    (df['Carbon Emission (ppm)'] > 350) &    # Higher carbon emissions\n",
    "    (df['hrock_type'].isin([2, 3]))          # Specific rock types\n",
    ")\n",
    "\n",
    "# Create the target column\n",
    "df['oil_mine_presence'] = np.where(conditions, 1, 0)\n",
    "\n",
    "# Display distribution of target labels\n",
    "print(df['oil_mine_presence'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ORK1aFrb9khB",
    "outputId": "8b5bcad5-08c2-42e0-dff8-7c26934d0c5d"
   },
   "outputs": [],
   "source": [
    "# !pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RhLtRA7i9UV-"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import torch\n",
    "# from torch_geometric.data import Data\n",
    "# from torch_geometric.nn import GATConv\n",
    "# from torch_geometric.loader import DataLoader\n",
    "# from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# # Encode categorical data\n",
    "# categorical_cols = ['hrock_type', 'arock_type', 'structure', 'orebody_fm']\n",
    "# label_encoders = {col: LabelEncoder() for col in categorical_cols}\n",
    "# for col in categorical_cols:\n",
    "#     df[col] = label_encoders[col].fit_transform(df[col])\n",
    "\n",
    "# # Feature selection\n",
    "# features = ['latitude', 'longitude', 'P Wave Velocity (km/s)', 'S Wave Velocity (km/s)',\n",
    "#              'Humidity (%)', 'Carbon Emission (ppm)', 'hrock_type', 'arock_type',\n",
    "#              'structure', 'orebody_fm']\n",
    "\n",
    "# df_features = df[features]\n",
    "# df_target = df['oil_mine_presence']  # Assuming this column exists\n",
    "\n",
    "# # Normalize numerical data\n",
    "# scaler = StandardScaler()\n",
    "# df_features = pd.DataFrame(scaler.fit_transform(df_features), columns=features)\n",
    "\n",
    "# # Graph Construction - Nodes and Edges\n",
    "# nodes = torch.tensor(df_features.values, dtype=torch.float)\n",
    "# edges = []\n",
    "\n",
    "# # Connect nodes based on proximity (example using Euclidean distance)\n",
    "# threshold_distance = 0.1  # Adjust based on your data\n",
    "# for i in range(len(df)):\n",
    "#     for j in range(i + 1, len(df)):\n",
    "#         dist = torch.norm(nodes[i, :2] - nodes[j, :2])\n",
    "#         if dist < threshold_distance:\n",
    "#             edges.append([i, j])\n",
    "#             edges.append([j, i])\n",
    "\n",
    "# tensor_edges = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# # Create Graph Data\n",
    "# graph_data = Data(x=nodes, edge_index=tensor_edges, y=torch.tensor(df_target.values, dtype=torch.long))\n",
    "\n",
    "# # GAT Model Definition\n",
    "# class GATModel(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "#         super(GATModel, self).__init__()\n",
    "#         self.gat1 = GATConv(in_channels, hidden_channels, heads=4, concat=True)\n",
    "#         self.gat2 = GATConv(hidden_channels * 4, out_channels, heads=1, concat=False)\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         x = torch.relu(self.gat1(x, edge_index))\n",
    "#         x = self.gat2(x, edge_index)\n",
    "#         return x\n",
    "\n",
    "# # Model Initialization\n",
    "# model = GATModel(in_channels=nodes.shape[1], hidden_channels=32, out_channels=2)  # Binary Classification\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# # DataLoader for training\n",
    "# data_list = [graph_data]\n",
    "# loader = DataLoader(data_list, batch_size=32, shuffle=True)\n",
    "\n",
    "# # Training Loop\n",
    "# def train():\n",
    "#     model.train()\n",
    "#     for data in loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         out = model(data.x, data.edge_index)\n",
    "#         loss = criterion(out, data.y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     print(f\"Training Loss: {loss.item():.4f}\")\n",
    "\n",
    "# # Evaluation\n",
    "# def evaluate():\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     for data in loader:\n",
    "#         out = model(data.x, data.edge_index)\n",
    "#         pred = out.argmax(dim=1)\n",
    "#         correct += int((pred == data.y).sum())\n",
    "#     acc = correct / len(df)\n",
    "#     print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "# # Run Training & Evaluation\n",
    "# for epoch in range(20):\n",
    "#     train()\n",
    "#     if epoch % 5 == 0:\n",
    "#         evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip list | grep torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lz2wKhd9CfM4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KTbKP2HnCheY"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load your dataframe (df) before proceeding\n",
    "\n",
    "# Encode categorical data\n",
    "categorical_cols = ['hrock_type', 'arock_type', 'structure', 'orebody_fm']\n",
    "label_encoders = {col: LabelEncoder() for col in categorical_cols}\n",
    "for col in categorical_cols:\n",
    "    df[col] = label_encoders[col].fit_transform(df[col])\n",
    "\n",
    "# Feature selection\n",
    "features = ['latitude', 'longitude', 'P Wave Velocity (km/s)', 'S Wave Velocity (km/s)',\n",
    "            'Humidity (%)', 'Carbon Emission (ppm)', 'hrock_type', 'arock_type',\n",
    "            'structure', 'orebody_fm']\n",
    "\n",
    "df_features = df[features]\n",
    "df_target = df['oil_mine_presence']  # Assuming this column exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "niKXB9W4Cj5l"
   },
   "outputs": [],
   "source": [
    "# # Normalize numerical data\n",
    "# scaler = StandardScaler()\n",
    "# df_features = pd.DataFrame(scaler.fit_transform(df_features), columns=features)\n",
    "\n",
    "# # Graph Construction - Nodes and Edges\n",
    "# nodes = torch.tensor(df_features.values, dtype=torch.float)\n",
    "# edges = []\n",
    "\n",
    "# # Connect nodes based on proximity (adjust threshold to prevent overfitting)\n",
    "# threshold_distance = 0.5  # Increased threshold to reduce over-connection\n",
    "# for i in range(len(df)):\n",
    "#     for j in range(i + 1, len(df)):\n",
    "#         dist = torch.norm(nodes[i, :2] - nodes[j, :2])\n",
    "#         if dist < threshold_distance:\n",
    "#             edges.append([i, j])\n",
    "#             edges.append([j, i])\n",
    "\n",
    "# tensor_edges = torch.tensor(edges, dtype=torch.long).t().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalize numerical data\n",
    "# scaler = StandardScaler()\n",
    "# df_features = pd.DataFrame(scaler.fit_transform(df_features), columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert to tensor\n",
    "# nodes = torch.tensor(df_features.values, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from scipy.spatial import KDTree\n",
    "\n",
    "# # Build KDTree for efficient neighbor search\n",
    "# tree = KDTree(nodes[:, :2])  # Use only the first 2 dimensions if spatial data\n",
    "\n",
    "# # Find neighbors within threshold\n",
    "# threshold_distance = 0.5\n",
    "# edges = []\n",
    "\n",
    "# for i in range(len(df)):\n",
    "#     indices = tree.query_ball_point(nodes[i, :2], threshold_distance)\n",
    "#     for j in indices:\n",
    "#         if i != j:  # Avoid self-loops\n",
    "#             edges.append([i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert to PyTorch tensor\n",
    "# tensor_edges = torch.tensor(edges, dtype=torch.long).t().contiguous()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below code works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Normalizing data...\n",
      "ðŸŒ³ Building KDTree... (this should be fast)\n",
      "ðŸ” Finding 10 nearest neighbors for each of 210772 nodes...\n",
      "ðŸ“Œ Constructing edges... This may take a few seconds.\n",
      "âœ… Processed 0/210772 nodes...\n",
      "âœ… Processed 10000/210772 nodes...\n",
      "âœ… Processed 20000/210772 nodes...\n",
      "âœ… Processed 30000/210772 nodes...\n",
      "âœ… Processed 40000/210772 nodes...\n",
      "âœ… Processed 50000/210772 nodes...\n",
      "âœ… Processed 60000/210772 nodes...\n",
      "âœ… Processed 70000/210772 nodes...\n",
      "âœ… Processed 80000/210772 nodes...\n",
      "âœ… Processed 90000/210772 nodes...\n",
      "âœ… Processed 100000/210772 nodes...\n",
      "âœ… Processed 110000/210772 nodes...\n",
      "âœ… Processed 120000/210772 nodes...\n",
      "âœ… Processed 130000/210772 nodes...\n",
      "âœ… Processed 140000/210772 nodes...\n",
      "âœ… Processed 150000/210772 nodes...\n",
      "âœ… Processed 160000/210772 nodes...\n",
      "âœ… Processed 170000/210772 nodes...\n",
      "âœ… Processed 180000/210772 nodes...\n",
      "âœ… Processed 190000/210772 nodes...\n",
      "âœ… Processed 200000/210772 nodes...\n",
      "âœ… Processed 210000/210772 nodes...\n",
      "ðŸ“¦ Converting edges to tensor format...\n",
      "âœ… Graph construction completed! ðŸŽ‰\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from scipy.spatial import KDTree\n",
    "\n",
    "# # Step 1: Normalize numerical data for better distance calculations\n",
    "# print(\"Normalizing data...\")\n",
    "# scaler = StandardScaler()\n",
    "# df_features = pd.DataFrame(scaler.fit_transform(df_features), columns=features)\n",
    "\n",
    "# # Convert to NumPy array (faster than DataFrame for KDTree)\n",
    "# nodes = df_features.values\n",
    "\n",
    "# # Step 2: Build KDTree (O(n log n) complexity)\n",
    "# print(\"Building KDTree... (this should be fast)\")\n",
    "# tree = KDTree(nodes)\n",
    "\n",
    "# # Step 3: Find nearest neighbors (efficient query)\n",
    "# k = 10  # Limit to 10 nearest neighbors per node\n",
    "# print(f\"Finding {k} nearest neighbors for each of {len(nodes)} nodes...\")\n",
    "\n",
    "# # Query all nodes at once (much faster than looping)\n",
    "# distances, indices = tree.query(nodes, k=k)\n",
    "\n",
    "# # Step 4: Construct graph edges\n",
    "# edges = []\n",
    "# print(\"Constructing edges... This may take a few seconds.\")\n",
    "\n",
    "# for i in range(len(indices)):\n",
    "#     if i % 10000 == 0:  # Print progress every 10,000 rows\n",
    "#         print(f\"Processed {i}/{len(nodes)} nodes...\")\n",
    "\n",
    "#     for j in indices[i]:\n",
    "#         if i != j:  # Avoid self-loops\n",
    "#             edges.append([i, j])\n",
    "\n",
    "# # Step 5: Convert edges to PyTorch tensor\n",
    "# print(\"ðŸ“¦ Converting edges to tensor format...\")\n",
    "# tensor_edges = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# print(\"Graph construction completed! ðŸŽ‰\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Processed 0/210772 nodes...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "# Build KDTree for efficient neighbor search\n",
    "# We are using the first two dimensions of the nodes for spatial data (e.g., X, Y coordinates)\n",
    "tree = KDTree(nodes[:, :2])  # Use only the first 2 dimensions if spatial data\n",
    "\n",
    "# Set the threshold for neighbor distance\n",
    "threshold_distance = 0.5\n",
    "\n",
    "# Initialize an empty list to store edges\n",
    "edges = []\n",
    "\n",
    "# Start iterating over each node (row) in the dataset\n",
    "for i in range(len(df)):\n",
    "    if i % 10000 == 0:  # Print progress for every 10,000 rows processed\n",
    "        print(f\"âœ… Processed {i}/{len(df)} nodes...\")\n",
    "\n",
    "    # Query the KDTree to find all neighbors of node `i` within the given threshold distance\n",
    "    indices = tree.query_ball_point(nodes[i, :2], threshold_distance)\n",
    "    \n",
    "    # Loop over the indices of the neighbors\n",
    "    for j in indices:\n",
    "        if i != j:  # Avoid self-loops (node connected to itself)\n",
    "            edges.append([i, j])  # Add the edge between nodes `i` and `j`\n",
    "\n",
    "# Print completion message after processing all rows\n",
    "print(f\"âœ… Completed edge construction! Total edges: {len(edges)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "IJQL-sV7C--X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([210772, 10])\n"
     ]
    }
   ],
   "source": [
    "# Create train-test split\n",
    "train_idx, test_idx = train_test_split(range(len(df)), test_size=0.2, random_state=42)\n",
    "\n",
    "# Create train and test masks\n",
    "train_mask = torch.zeros(len(df), dtype=torch.bool)\n",
    "test_mask = torch.zeros(len(df), dtype=torch.bool)\n",
    "train_mask[train_idx] = True\n",
    "test_mask[test_idx] = True\n",
    "\n",
    "# Create Graph Data object\n",
    "graph_data = Data(\n",
    "    x=torch.tensor(nodes, dtype=torch.float),  # Convert to tensor\n",
    "    edge_index=tensor_edges,\n",
    "    y=torch.tensor(df_target.values, dtype=torch.long),\n",
    "    train_mask=train_mask,\n",
    "    test_mask=test_mask\n",
    ")\n",
    "\n",
    "print(type(graph_data.x))  # Should be <class 'torch.Tensor'>\n",
    "print(graph_data.x.shape)  # Should match your input feature size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "sR6HSlaQDMB_"
   },
   "outputs": [],
   "source": [
    "\n",
    "# GAT Model Definition\n",
    "class GATModel(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GATModel, self).__init__()\n",
    "        self.gat1 = GATConv(in_channels, hidden_channels, heads=4, concat=True, dropout=0.6)\n",
    "        self.gat2 = GATConv(hidden_channels * 4, out_channels, heads=1, concat=False, dropout=0.6)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        if isinstance(x, tuple):  # Ensure x is not a tuple\n",
    "            x = x[0]\n",
    "        x = torch.relu(self.gat1(x, edge_index))\n",
    "        x = self.gat2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Model Initialization\n",
    "model = GATModel(in_channels=nodes.shape[1], hidden_channels=32, out_channels=2)  # Binary Classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# DataLoader for training\n",
    "data_list = [graph_data]\n",
    "loader = DataLoader(data_list, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1izAAaCvDNXD"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Training Loop with only train data\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(graph_data.x, graph_data.edge_index)\n",
    "    loss = criterion(out[graph_data.train_mask], graph_data.y[graph_data.train_mask])  # Use only training data\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Training Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "1HW863VcAi0f"
   },
   "outputs": [],
   "source": [
    "# Evaluation with test data\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    out = model(graph_data.x, graph_data.edge_index)\n",
    "    pred = out.argmax(dim=1)\n",
    "    correct = (pred[graph_data.test_mask] == graph_data.y[graph_data.test_mask]).sum().item()\n",
    "    acc = correct / graph_data.test_mask.sum().item()\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ZzYfmp8EC8oq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.7496\n",
      "Test Accuracy: 0.9187\n",
      "Training Loss: 0.4719\n",
      "Training Loss: 0.3079\n",
      "Training Loss: 0.2152\n",
      "Training Loss: 0.1592\n",
      "Training Loss: 0.1254\n",
      "Test Accuracy: 1.0000\n",
      "Training Loss: 0.1032\n",
      "Training Loss: 0.0887\n",
      "Training Loss: 0.0778\n",
      "Training Loss: 0.0700\n",
      "Training Loss: 0.0641\n",
      "Test Accuracy: 1.0000\n",
      "Training Loss: 0.0596\n",
      "Training Loss: 0.0558\n",
      "Training Loss: 0.0531\n",
      "Training Loss: 0.0498\n",
      "Training Loss: 0.0467\n",
      "Test Accuracy: 1.0000\n",
      "Training Loss: 0.0436\n",
      "Training Loss: 0.0410\n",
      "Training Loss: 0.0386\n",
      "Training Loss: 0.0368\n"
     ]
    }
   ],
   "source": [
    "# Run Training & Evaluation\n",
    "for epoch in range(20):\n",
    "    train()\n",
    "    if epoch % 5 == 0:\n",
    "        evaluate()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
